{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8e9c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your journey starts with one step.\n",
      "Your - poss - journey\n",
      "journey - nsubj - starts\n",
      "starts - ROOT - starts\n",
      "with - prep - starts\n",
      "one - nummod - step\n",
      "step - pobj - with\n",
      ". - punct - starts\n",
      "\n",
      "\n",
      "but i dont like you.\n",
      "but - cc - like\n",
      "i - nsubj - like\n",
      "do - aux - like\n",
      "nt - neg - like\n",
      "like - ROOT - like\n",
      "you - dobj - like\n",
      ". - punct - like\n",
      "\n",
      "\n",
      "wow wow amazing.\n",
      "wow - intj - amazing\n",
      "wow - intj - amazing\n",
      "amazing - ROOT - amazing\n",
      ". - punct - amazing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "f = nlp(\"Your journey starts with one step. but i dont like you. wow wow amazing.\")\n",
    "\n",
    "for sent in f.sents:\n",
    "    print(sent.text)\n",
    "    for token in sent:\n",
    "        print(f\"{token.text} - {token.dep_} - {token.head.text}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ffa726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47871c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "# converts the words in word_tokens to lower case and then checks whether \n",
    "#they are present in stop_words or not\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "#with no lower case conversion\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f423c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\selva tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['example', 'sentence', 'demonstrating', 'tokenization', 'stopword', 'removal']\n",
      "POS Tags: [('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('demonstrating', 'VBG'), ('tokenization', 'NN'), ('and', 'CC'), ('stopword', 'NN'), ('removal', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import pos_tag \n",
    "import string \n",
    "# Download necessary resources \n",
    "nltk.download('punkt_tab') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger_eng') \n",
    "def preprocess_text(text): \n",
    "    # Tokenize text \n",
    "    tokens = word_tokenize(text) \n",
    "    # Convert to lowercase and remove punctuation \n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()] \n",
    "    # Remove stopwords \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words] \n",
    "    return filtered_tokens \n",
    "\n",
    "def pos_tagging(text): \n",
    "\n",
    "    tokens = word_tokenize(text) \n",
    "    return pos_tag(tokens) \n",
    "    # Example usage \n",
    "text = \"This is an example sentence demonstrating tokenization and stopword removal.\" \n",
    "print(\"Filtered Tokens:\", preprocess_text(text)) \n",
    "print(\"POS Tags:\", pos_tagging(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c923cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Vocabulary: ['.', 'implementation', 'word2vec', 'provides', 'gensim', 'useful', 'are', 'embeddings', 'word', 'sentence', 'sample', 'a', 'is', 'this']\n",
      "\n",
      "Word Embeddings:\n",
      "'.': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "'implementation': [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      "'word2vec': [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "'provides': [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "  4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "  6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "  3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "  8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "  1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      " -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "  2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      " -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "  9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "  3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "  7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      " -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      " -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "  1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "  2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "  4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      " -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      " -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      " -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      " -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      " -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      " -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      " -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "  5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
      "'gensim': [-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464722\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310898  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058446\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475375 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138926 -0.00731732 -0.00969783 -0.00908026 -0.00102275 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339215 -0.00097922\n",
      "  0.00997913  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.0026418  -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n",
      "'useful': [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n",
      "'are': [ 8.13227147e-03 -4.45733406e-03 -1.06835726e-03  1.00636482e-03\n",
      " -1.91113955e-04  1.14817743e-03  6.11386076e-03 -2.02715401e-05\n",
      " -3.24596534e-03 -1.51072862e-03  5.89729892e-03  1.51410222e-03\n",
      " -7.24261976e-04  9.33324732e-03 -4.92128357e-03 -8.38409644e-04\n",
      "  9.17541143e-03  6.74942741e-03  1.50285603e-03 -8.88256077e-03\n",
      "  1.14874600e-03 -2.28825561e-03  9.36823711e-03  1.20992784e-03\n",
      "  1.49006362e-03  2.40640994e-03 -1.83600665e-03 -4.99963388e-03\n",
      "  2.32429506e-04 -2.01418041e-03  6.60093315e-03  8.94012302e-03\n",
      " -6.74754381e-04  2.97701475e-03 -6.10765442e-03  1.69932481e-03\n",
      " -6.92623248e-03 -8.69402662e-03 -5.90020278e-03 -8.95647518e-03\n",
      "  7.27759488e-03 -5.77203138e-03  8.27635173e-03 -7.24354526e-03\n",
      "  3.42167495e-03  9.67499893e-03 -7.78544787e-03 -9.94505733e-03\n",
      " -4.32914635e-03 -2.68313056e-03 -2.71289347e-04 -8.83155130e-03\n",
      " -8.61755759e-03  2.80021061e-03 -8.20640661e-03 -9.06933658e-03\n",
      " -2.34046578e-03 -8.63180775e-03 -7.05664977e-03 -8.40115082e-03\n",
      " -3.01328895e-04 -4.56429832e-03  6.62717456e-03  1.52716041e-03\n",
      " -3.34147573e-03  6.10897178e-03 -6.01328490e-03 -4.65616956e-03\n",
      " -7.20750913e-03 -4.33658017e-03 -1.80932996e-03  6.48964290e-03\n",
      " -2.77039292e-03  4.91896737e-03  6.90444233e-03 -7.46370573e-03\n",
      "  4.56485013e-03  6.12697843e-03 -2.95447465e-03  6.62502181e-03\n",
      "  6.12587947e-03 -6.44348515e-03 -6.76455162e-03  2.53895880e-03\n",
      " -1.62381888e-03 -6.06512791e-03  9.49920900e-03 -5.13014663e-03\n",
      " -6.55409694e-03 -1.19885204e-04 -2.70142802e-03  4.44400299e-04\n",
      " -3.53745813e-03 -4.19330609e-04 -7.08615757e-04  8.22820642e-04\n",
      "  8.19481723e-03 -5.73670724e-03 -1.65952800e-03  5.57160750e-03]\n",
      "'embeddings': [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
      " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
      " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
      "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
      "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
      "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
      " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
      " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
      " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
      " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
      "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
      "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
      "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
      "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
      " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
      "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
      "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
      " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
      "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
      "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
      " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
      "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
      "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
      " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
      " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
      "'word': [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
      "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
      " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
      "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
      " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
      "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
      "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
      "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
      "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
      " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
      " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
      " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
      " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
      " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
      "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
      " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
      " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
      " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
      "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
      " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
      " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
      "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
      " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
      "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
      "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n",
      "'sentence': [-0.00515653 -0.0066687  -0.00777726  0.00831118 -0.00198245 -0.00685534\n",
      " -0.00415462  0.00514441 -0.00286929 -0.00374987  0.00162151 -0.00277645\n",
      " -0.00158445  0.00107455 -0.00297811  0.00851975  0.00391115 -0.00995941\n",
      "  0.00625994 -0.00675462  0.00076948  0.00440448 -0.00510365 -0.00211079\n",
      "  0.00809592 -0.00424403 -0.00763668  0.00925842 -0.00215562 -0.00471969\n",
      "  0.00857127  0.00428357  0.00432507  0.00928502 -0.00845354  0.00525561\n",
      "  0.00203946  0.00418851  0.00169799  0.00446438  0.00448654  0.00610486\n",
      " -0.00320227 -0.00457598 -0.00042654  0.00253387 -0.00326335  0.00605805\n",
      "  0.00415436  0.00776502  0.00256941  0.00811713 -0.00138729  0.00807837\n",
      "  0.00371722 -0.00804777 -0.00393383 -0.00247201  0.00489331 -0.00087221\n",
      " -0.00283106  0.00783414  0.00932341 -0.00161502 -0.00515953 -0.00470202\n",
      " -0.00484632 -0.00960335  0.0013721  -0.00422515  0.00252685  0.00561479\n",
      " -0.00406613 -0.00959711  0.00154678 -0.00670049  0.00249531 -0.00378084\n",
      "  0.00707881  0.00064026  0.00356113 -0.00273928 -0.00171064  0.00765321\n",
      "  0.00140776 -0.00585077 -0.00783493  0.00123275  0.00645499  0.00555665\n",
      " -0.00897754  0.00859264  0.0040472   0.00747001  0.00974687 -0.00728998\n",
      " -0.00904046  0.00583632  0.00939173  0.00350712]\n",
      "'sample': [ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n",
      " -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n",
      " -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n",
      "  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n",
      "  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n",
      " -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n",
      " -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n",
      "  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n",
      "  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n",
      "  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n",
      " -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n",
      " -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n",
      " -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n",
      " -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n",
      " -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n",
      " -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n",
      " -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n",
      "  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n",
      " -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n",
      "  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n",
      "  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n",
      " -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n",
      "  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n",
      " -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n",
      "  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n",
      "'a': [ 9.7703049e-03  8.1651295e-03  1.2809900e-03  5.0975592e-03\n",
      "  1.4081334e-03 -6.4551458e-03 -1.4280414e-03  6.4491532e-03\n",
      " -4.6172994e-03 -3.9930567e-03  4.9244007e-03  2.7131049e-03\n",
      " -1.8479716e-03 -2.8769460e-03  6.0107387e-03 -5.7167588e-03\n",
      " -3.2367117e-03 -6.4878017e-03 -4.2346469e-03 -8.5809790e-03\n",
      " -4.4697910e-03 -8.5112397e-03  1.4037896e-03 -8.6181918e-03\n",
      " -9.9166743e-03 -8.2016150e-03 -6.7726481e-03  6.6805636e-03\n",
      "  3.7845615e-03  3.5617739e-04 -2.9580018e-03 -7.4283308e-03\n",
      "  5.3340854e-04  4.9987051e-04  1.9563863e-04  8.5258327e-04\n",
      "  7.8632595e-04 -6.8170091e-05 -8.0070579e-03 -5.8702836e-03\n",
      " -8.3829220e-03 -1.3120568e-03  1.8206445e-03  7.4171387e-03\n",
      " -1.9634261e-03 -2.3252976e-03  9.4871623e-03  7.9690355e-05\n",
      " -2.4045315e-03  8.6048292e-03  2.6869976e-03 -5.3439913e-03\n",
      "  6.5881093e-03  4.5101345e-03 -7.0544761e-03 -3.2315517e-04\n",
      "  8.3449570e-04  5.7473630e-03 -1.7176659e-03 -2.8065280e-03\n",
      "  1.7484374e-03  8.4715319e-04  1.1928055e-03 -2.6342785e-03\n",
      " -5.9857722e-03  7.3229950e-03  7.5873868e-03  8.2963798e-03\n",
      " -8.5988501e-03  2.6364352e-03 -3.5599684e-03  9.6203908e-03\n",
      "  2.9037774e-03  4.6411357e-03  2.3856114e-03  6.6084936e-03\n",
      " -5.7432964e-03  7.8944210e-03 -2.4109385e-03 -4.5618871e-03\n",
      " -2.0609987e-03  9.7335642e-03 -6.8565863e-03 -2.1917380e-03\n",
      "  7.0009963e-03 -5.5735734e-05 -6.2949490e-03 -6.3935285e-03\n",
      "  8.9403801e-03  6.4295628e-03  4.7736140e-03 -3.2620677e-03\n",
      " -9.2676291e-03  3.7868707e-03  7.1605276e-03 -5.6328722e-03\n",
      " -7.8649912e-03 -2.9727537e-03 -4.9319202e-03 -2.3151194e-03]\n",
      "'is': [-1.9442164e-03 -5.2675214e-03  9.4471136e-03 -9.2987325e-03\n",
      "  4.5039477e-03  5.4041781e-03 -1.4092624e-03  9.0070926e-03\n",
      "  9.8853596e-03 -5.4750429e-03 -6.0210000e-03 -6.7469729e-03\n",
      " -7.8948820e-03 -3.0479168e-03 -5.5940272e-03 -8.3446801e-03\n",
      "  7.8290224e-04  2.9946566e-03  6.4147436e-03 -2.6289499e-03\n",
      " -4.4534765e-03  1.2495709e-03  3.9146186e-04  8.1169987e-03\n",
      "  1.8280029e-04  7.2315861e-03 -8.2645155e-03  8.4335366e-03\n",
      " -1.8889094e-03  8.7011540e-03 -7.6168370e-03  1.7963862e-03\n",
      "  1.0564864e-03  4.6005251e-05 -5.1032533e-03 -9.2476979e-03\n",
      " -7.2642174e-03 -7.9511739e-03  1.9137275e-03  4.7846674e-04\n",
      " -1.8131376e-03  7.1201660e-03 -2.4756920e-03 -1.3473093e-03\n",
      " -8.9005642e-03 -9.9254129e-03  8.9493981e-03 -5.7539381e-03\n",
      " -6.3729975e-03  5.1994072e-03  6.6699935e-03 -6.8316413e-03\n",
      "  9.5975993e-04 -6.0084737e-03  1.6473436e-03 -4.2892788e-03\n",
      " -3.4407973e-03  2.1856665e-03  8.6615775e-03  6.7281104e-03\n",
      " -9.6770572e-03 -5.6221043e-03  7.8803329e-03  1.9893574e-03\n",
      " -4.2560520e-03  5.9881213e-04  9.5209610e-03 -1.1027169e-03\n",
      " -9.4246380e-03  1.6084099e-03  6.2323548e-03  6.2823701e-03\n",
      "  4.0916502e-03 -5.6502391e-03 -3.7069322e-04 -5.5317880e-05\n",
      "  4.5717955e-03 -8.0415895e-03 -8.0183093e-03  2.6475071e-04\n",
      " -8.6082993e-03  5.8201565e-03 -4.1781188e-04  9.9711772e-03\n",
      " -5.3439774e-03 -4.8613906e-04  7.7567734e-03 -4.0679323e-03\n",
      " -5.0159004e-03  1.5900708e-03  2.6506938e-03 -2.5649595e-03\n",
      "  6.4475285e-03 -7.6599526e-03  3.3935606e-03  4.8997044e-04\n",
      "  8.7321829e-03  5.9827138e-03  6.8153618e-03  7.8225443e-03]\n",
      "'this': [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
      " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
      " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
      "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
      "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
      "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
      " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
      "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
      "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
      " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
      "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
      "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
      " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
      "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
      "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
      "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
      "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\selva tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from gensim.models import Word2Vec \n",
    "import string \n",
    "\n",
    "nltk.download('punkt_tab') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger_eng') \n",
    "def preprocess_text(text): \n",
    "    # Tokenize text \n",
    "    tokens = word_tokenize(text) \n",
    "    # Convert to lowercase and remove punctuation \n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()] \n",
    "    # Remove stopwords \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words] \n",
    "    return filtered_tokens \n",
    "\n",
    "def train_word2vec(corpus): \n",
    "    sentences = [word_tokenize(sentence.lower()) for sentence in corpus] \n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4) \n",
    "    return model \n",
    "# Example Word2Vec training \n",
    "corpus = [\"This is a sample sentence.\", \"Word embeddings are useful.\", \"Gensim provides Word2Vec implementation.\"] \n",
    "word2vec_model = train_word2vec(corpus) \n",
    "print(\"Word2Vec Vocabulary:\", list(word2vec_model.wv.index_to_key)) \n",
    "# Get vectors for all words in the vocabulary \n",
    "print(\"\\nWord Embeddings:\") \n",
    "for word in word2vec_model.wv.index_to_key: \n",
    "    print(f\"'{word}': {word2vec_model.wv[word]}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10e14b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\selva\n",
      "[nltk_data]     tharrun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading averaged_perceptron_tagger_out: Package\n",
      "[nltk_data]     'averaged_perceptron_tagger_out' not found in index\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Example Doc2Vec training \u001b[39;00m\n\u001b[32m     24\u001b[39m documents = [\u001b[33m\"\u001b[39m\u001b[33mDoc2Vec helps in document embedding.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIt is useful for NLP tasks.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGensim provides easy implementation.\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m doc2vec_model = \u001b[43mtrain_doc2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDoc2Vec Model Trained Successfully\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEmbeddings for Training Documents:\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_doc2vec\u001b[39m\u001b[34m(paragraphs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, paragraph \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paragraphs):  \n\u001b[32m     19\u001b[39m     tagged_data = TaggedDocument(words=word_tokenize(paragraph.lower()), tags=[\u001b[38;5;28mstr\u001b[39m(i)]) \n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     model = \u001b[43mDoc2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagged_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Default is dm=1 (Distributed Memory). You can also use dbow_words=1 or a combination. \u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\selva tharrun\\LLM-from-scratch\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001b[39m, in \u001b[36mDoc2Vec.__init__\u001b[39m\u001b[34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28mself\u001b[39m.dv.vectors_lockf = np.ones(\u001b[32m1\u001b[39m, dtype=REAL)  \u001b[38;5;66;03m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDoc2Vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43msg\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnull_word\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdm_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshrink_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\selva tharrun\\LLM-from-scratch\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[39m, in \u001b[36mWord2Vec.__init__\u001b[39m\u001b[34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=(epochs + \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m.train(\n\u001b[32m    431\u001b[39m         corpus_iterable=corpus_iterable, corpus_file=corpus_file, total_examples=\u001b[38;5;28mself\u001b[39m.corpus_count,\n\u001b[32m    432\u001b[39m         total_words=\u001b[38;5;28mself\u001b[39m.corpus_total_words, epochs=\u001b[38;5;28mself\u001b[39m.epochs, start_alpha=\u001b[38;5;28mself\u001b[39m.alpha,\n\u001b[32m    433\u001b[39m         end_alpha=\u001b[38;5;28mself\u001b[39m.min_alpha, compute_loss=\u001b[38;5;28mself\u001b[39m.compute_loss, callbacks=callbacks)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\selva tharrun\\LLM-from-scratch\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[39m, in \u001b[36mDoc2Vec.build_vocab\u001b[39m\u001b[34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_vocab\u001b[39m(\n\u001b[32m    842\u001b[39m         \u001b[38;5;28mself\u001b[39m, corpus_iterable=\u001b[38;5;28;01mNone\u001b[39;00m, corpus_file=\u001b[38;5;28;01mNone\u001b[39;00m, update=\u001b[38;5;28;01mFalse\u001b[39;00m, progress_per=\u001b[32m10000\u001b[39m,\n\u001b[32m    843\u001b[39m         keep_raw_vocab=\u001b[38;5;28;01mFalse\u001b[39;00m, trim_rule=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs,\n\u001b[32m    844\u001b[39m     ):\n\u001b[32m    845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[32m    846\u001b[39m \n\u001b[32m    847\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    880\u001b[39m \n\u001b[32m    881\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     total_words, corpus_count = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_vocab\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    886\u001b[39m     \u001b[38;5;28mself\u001b[39m.corpus_count = corpus_count\n\u001b[32m    887\u001b[39m     \u001b[38;5;28mself\u001b[39m.corpus_total_words = total_words\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\selva tharrun\\LLM-from-scratch\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[39m, in \u001b[36mDoc2Vec.scan_vocab\u001b[39m\u001b[34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1052\u001b[39m     corpus_iterable = TaggedLineDocument(corpus_file)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m total_words, corpus_count = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scan_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_per\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim_rule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m logger.info(\n\u001b[32m   1057\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m word types and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m unique tags from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m examples and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m words\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw_vocab), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dv), corpus_count, total_words,\n\u001b[32m   1059\u001b[39m )\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\selva tharrun\\LLM-from-scratch\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:956\u001b[39m, in \u001b[36mDoc2Vec._scan_vocab\u001b[39m\u001b[34m(self, corpus_iterable, progress_per, trim_rule)\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document_no, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corpus_iterable):\n\u001b[32m    955\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checked_string_types:\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mdocument\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    957\u001b[39m             logger.warning(\n\u001b[32m    958\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mEach \u001b[39m\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m\u001b[33m should be a list of words (usually unicode strings). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    959\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFirst \u001b[39m\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m\u001b[33m here is instead plain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    960\u001b[39m                 \u001b[38;5;28mtype\u001b[39m(document.words),\n\u001b[32m    961\u001b[39m             )\n\u001b[32m    962\u001b[39m         checked_string_types += \u001b[32m1\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from gensim.models import Doc2Vec \n",
    "from gensim.models.doc2vec import TaggedDocument \n",
    "import string \n",
    "\n",
    "nltk.download('punkt_tab') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger_out') \n",
    "def preprocess_text(text):       \n",
    "    tokens = word_tokenize(text) \n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()] \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words] \n",
    "    return filtered_tokens \n",
    "def train_doc2vec(paragraphs): \n",
    "    for i, paragraph in enumerate(paragraphs):  \n",
    "        tagged_data = TaggedDocument(words=word_tokenize(paragraph.lower()), tags=[str(i)]) \n",
    "        model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4, epochs=20) \n",
    "    # Default is dm=1 (Distributed Memory). You can also use dbow_words=1 or a combination. \n",
    "    return model \n",
    "# Example Doc2Vec training \n",
    "documents = [\"Doc2Vec helps in document embedding.\", \"It is useful for NLP tasks.\", \"Gensim provides easy implementation.\"] \n",
    "doc2vec_model = train_doc2vec(documents) \n",
    "print(\"Doc2Vec Model Trained Successfully\") \n",
    "print(\"\\nEmbeddings for Training Documents:\") \n",
    "\n",
    "for i, doc in enumerate(documents): \n",
    "    # Access the vector using the tag assigned during training \n",
    "    doc_vector = doc2vec_model.dv[str(i)] \n",
    "print(f\"Document {i+1}: {doc_vector[:10]}...\") # Print first 10 dimensions for brevity "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
